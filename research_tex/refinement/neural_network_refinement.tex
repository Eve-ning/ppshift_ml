\begin{document}

\section{Refining Neural Network}

In this section, we will be talking about how the neural network goes through change to score best.

\subsection{Initial Draft}

For this neural network, we will firstly start off with the following code. A simple neural network, with 2 layers, the input, and the output.

\begin{lstlisting}[language=Python]

def model_c():
    
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(<neurons>, input_shape=(12,), \
		kernel_initializer='normal', activation='relu'))
    model.add(keras.layers.Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer='adam')
   
    return model

\end{lstlisting}

\paragraph{input\_shape} This defines how many dimensions we have the input as, we have 12 parameters, hence 12.

\paragraph{kernal\_initializer} This defines how the weights are randomly allocated before the learning process. For this we just use the normal.

\paragraph{activation} We will simply be using \textbf{ReLU} (rectified linear unit) as our activation function. This simply maps any positive values in the neuron to a linear function, anything non-positive is mapped to 0.

\paragraph{loss} The loss describes how far the machine is predicting all values correctly. In this case, we will use mean\_squared\_error as our minimum loss target as we are dealing with a \textit{regression} problem.

\paragraph{optimizer} It's hard to describe what is \textbf{Adam} (adaptive moment estimation), however, it is described to be "better" than \textbf{SGD} (Stochastic Gradient Descent). More info here: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/


\end{document}
