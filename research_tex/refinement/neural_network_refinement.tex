\begin{document}

\section{Refining Neural Network}

In this section, we will be talking about how the neural network goes through change to score best.

\subsection{Initial Draft}

For this neural network, we will firstly start off with the following code. A simple neural network, with 2 layers, the input, and the output.

\begin{lstlisting}[language=Python]

def model_c():
    
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(<neurons>, input_shape=(12,), \
		kernel_initializer='normal', activation='relu'))
    model.add(keras.layers.Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer='adam')
   
    return model

\end{lstlisting}

\paragraph{input\_shape} This defines how many dimensions we have the input as, we have 12 parameters, hence 12.

\paragraph{kernal\_initializer} This defines how the weights are randomly allocated before the learning process. For this we just use the normal.

\paragraph{activation} We will simply be using \textbf{ReLU} (rectified linear unit) as our activation function. This simply maps any positive values in the neuron to a linear function, anything non-positive is mapped to 0.

\paragraph{loss} The loss describes how far the machine is predicting all values correctly. In this case, we will use mean\_squared\_error as our minimum loss target as we are dealing with a \textit{regression} problem.

\paragraph{optimizer} It's hard to describe what is \textbf{Adam} (adaptive moment estimation), however, it is described to be "better" than \textbf{SGD} (Stochastic Gradient Descent). More info here: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/

\subsubsection{Layer Improvements}

We will keep it short on the guess and check for neuron layers, here are the results and improvements done.

\begin{tabular}{|c|c|c|c|c|c|c|c|} 
\hline
\multicolumn{6}{|c|}{Parameters} & \multicolumn{2}{|c|}{Loss}  \\
\hline
\# & $L_1$ & $L_2$ & $L_3$ & Epochs & Batch Size & Mean & STDEV \\
\hline
1 & 96 & 48 & 24 & 10 & 500 & 1.80 & 0.60 \\ \hline
\multicolumn{8}{|c|}{Drop Batch Size to 200}  \\ \hline
2 & 96 & 48 & 24 & 10 & 200 & 1.81 & 0.58 \\ \hline
\multicolumn{8}{|c|}{Half Neurons}  \\ \hline
3 & 48 & 24 & 12 & 10 & 200 & 1.81 & 0.61 \\ \hline
\multicolumn{8}{|c|}{Add Reg L1 (0.01) on $L_2$ and $L_3$}  \\ \hline
4 & 96 & 48 & 24 & 10 & 200 & 1.67 & 0.67  \\ \hline
5 & {} & $L1_{0.01}$ & $L1_{0.01}$ & {} & {} & {} & {} \\ \hline
\multicolumn{8}{|c|}{Add Reg L1 (0.02) on $L_2$ and $L_3$}  \\ \hline
6 & 96 & 48 & 24 & 10 & 200 & 1.73 & 0.63  \\ \hline
{} & {} & $L1_{0.02}$ & $L1_{0.02}$ & {} & {} & {} & {} \\ \hline
\multicolumn{8}{|c|}{Add Reg L2 (0.01) on $L_2$ and $L_3$}  \\ \hline
7 & 96 & 48 & 24 & 10 & 200 & 1.72 & 0.64  \\ \hline
{} & {} & $L2_{0.01}$ & $L2_{0.01}$ & {} & {} & {} & {} \\ \hline
\multicolumn{8}{|c|}{Add Dropout (0.25) after $L_2$ and $L_3$}  \\ \hline
8 & 96 & 48 & 24 & 10 & 200 & 1.73 & 0.63  \\ \hline
{} & {} & $L1_{0.01}$ & $L1_{0.01}$ & {} & {} & {} & {} \\ \hline
{} & {} & $DO_{0.25}$ & $L2_{0.25}$ & {} & {} & {} & {} \\ \hline
\multicolumn{8}{|c|}{Revert to 7, increased Epoch and Decreased Batch Size}  \\ \hline
9 & 96 & 48 & 24 & 50 & 50 & 1.69 & 0.56  \\ \hline
{} & {} & $L2_{0.01}$ & $L2_{0.01}$ & {} & {} & {} & {} \\ \hline
\multicolumn{8}{|c|}{Increase Reg L2 to 0.03}  \\ \hline
10 & 96 & 48 & 24 & 50 & 50 & 1.81 & 0.57  \\ \hline
{} & {} & $L2_{0.03}$ & $L2_{0.03}$ & {} & {} & {} & {} \\ \hline
\multicolumn{8}{|c|}{Revert to 7, doubled neurons}  \\ \hline
11 & 192 & 96 & 48 & 50 & 50 & 1.75 & 0.57  \\ \hline
{} & {} & $L2_{0.03}$ & $L2_{0.03}$ & {} & {} & {} & {} \\ \hline
\end{tabular}

In the end of the experimenting, we will settle on \textbf{Model 9}	

\end{document}
