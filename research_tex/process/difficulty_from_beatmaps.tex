\begin{document}
\section{Difficulty from Beatmaps}

We turn our attention to how we can figure out difficulty from the map itself, the expected output we want would be:

$$ difficulty := \lbrace(offset_1, difficulty_1), (offset_2, difficulty_2), ..., (offset_n, difficulty_n)\rbrace $$

Whereby we estimate difficulty at offset \textbf{n} from the map itself:

$$ difficulty_n \approx model \left( reading_n, \sum_{k=1}^{keys} \left(strain_k \right), ... \right)$$

There are more factors (denoted by $...$) that contribute to difficulty, but we will regard them as noise in this research and fine tune this equation later.

\paragraph{Reading} This denotes how hard is it to read all the patterns on the screen. We can draw similarities between this and density, however density focuses a lot more on its previous and future surroundings where reading looks at the future ones only. 

\paragraph{Density} This focuses on the \textbf{imminent} density of the offset. contrary to strain, it disregards the global trends of patterns. We will not use this in our network as strain does a better job in calculation.

\paragraph{Strain} This is reliant on $density$ whereby continuous high values of $ density$ will result in a high $strain$. This has an additional hyperparameter, $decay$, where it denotes how fast the player can recover from $strain_n$. Finger $strain$ on the same hand will likely affect the other $strain$ values of the other fingers.

\subsection{Note Type Weights}

This will define the \textbf{weightages} of each note type.
\paragraph{$weight_{NN}$} defines for normal notes
\paragraph{$weight_{LNh}$} defines for long notes heads
\paragraph{$weight_{LNt}$} defines for long notes tails
\paragraph{$weight_{SSh}$} defines for \textbf{strain shift} for hands \textbf{(explained later)}
\paragraph{$weight_{SSb}$} defines for \textbf{strain shift} for body \textbf{(explained later)}

\subsection{Reading}

$$ reading_{(n,n+\theta)} := 
count(NN), count(LNh), count(LNt)
= \lbrace n \leq offset \leq (n+ \theta) \rbrace$$

Where,

\paragraph{$n$} is the initial offset
\paragraph{$\theta$} is the hyperparameter for length.
We will not take into consideration the length of $note_{long}$

\subsection{Density}

We will look into density before strain as it's derived from this.

Considering the notes on the $k$ column
$$ \lbrace ..., n-2, n-1, n, n+1, n+2, ... \rbrace $$
$$ \Delta_{nx}^k = \frac{1}{|n - x|}$$
$$ density_n^k =
\sum_{N=n-\sigma}^{n+\sigma}
\left(
\Delta_{nN}^k
\right)$$

So for $\sigma = 2$ and 
$$ column_k := \lbrace a, b, n, d, e\rbrace$$
$$ density_n^k = \Delta_{na}^k + \Delta_{nb}^k + \Delta_{nd}^k + \Delta_{ne}^k $$
$$ density_n^k = \frac{1}{|n-a|} + \frac{1}{|n-b|} + \frac{1}{|n-d|} + \frac{1}{|n-e|}$$

\paragraph{$\Delta_{nx}^k$} will be the the inverse of the (ms) distance between notes $n$ and $x$ on column $k$. Notes that are further away will be penalized more heavily.

\paragraph{$\sigma$} defines the range, front and back of the search. Higher sigma may prove to be useless with further $\Delta_{nx}^k$ being too small.

\subsection{Strain}

This will work in relationship with $density$, whereby a $strain$ is a cumulative function of $density$ with a \textbf{linear decay function}.

Notes:
\begin{enumerate}
	\item Better players have \textbf{higher decay gradients}
	\item If $decay > density$, $strain$ will \textbf{decrease}
	\item If $decay < density$, $strain$ will \textbf{increase}
	\item There will be a point where $strain$ is high enough to affect physical performance, indirectly affecting accuracy.
\end{enumerate}

\subsubsection{Strain Shift}

Strain will not only affect one finger, it will affect the hand and both after time, just on a smaller scale

\paragraph{Hand} We will denote the strain shift hyperparameter of one finger to another on the same hand to be $SS_H$
\paragraph{Body} Likewise, for body, we will denote as $SS_B$

\subsubsection{Strain Example}

Consider the case, without \textbf{Strain Shift}
$$ Where, weight_{NN} = 1, \sigma = 2 $$
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|} 
	\hline
	2500 & 0 			& 0 & 0 &       & 0.022 & 0.016\\ \hline
	2000 & $weight_{NN}$& 0 & 0 & 0.003 & 0.022 & 0.017\\	\hline
	1500 & $weight_{NN}$& 0 & 0 & 0.005 & 0.019 & 0.015\\	\hline
	1000 & $weight_{NN}$& 0 & 0 & 0.006 & 0.014 & 0.011\\	\hline
	 500 & $weight_{NN}$& 0 & 0 & 0.005 & 0.008 & 0.006\\	\hline
	   0 & $weight_{NN}$& 0 & 0 & 0.003 & 0.003 & 0.002\\	\hline
    -500 & 0 			& 0 & 0 & 		& 0	 	& 0	\\	\hline
   -1000 & 0 			& 0 & 0 & 		& 0	 	& 0	\\
	\hline
	Offset(ms) & k=1 & k=2 & k=3 & $\approx Density$ & Strain (dec=0) & Strain (dec=0.001) \\ 
	\hline
\end{tabular}
\end{center}

Consider the case, with \textbf{Strain Shift}
\begin{center}
	\begin{tabular}{|c|c|c|c|} 
	\hline
	2500 & 0			  & 0 		& 0 	\\ \hline
	2000 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	1500 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	1000 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	 500 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	   0 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
    -500 & 0			  & 0 		& 0 	\\	\hline
   -1000 & 0			  & 0 		& 0 	\\
	\hline
	Offset(ms) & k=1 & k=2 & k=3\\ 
	\hline
\end{tabular}
\end{center}

It's hard to include the calculations in the table, so we'll look at $density_{(1,1000)}$.

$$density_{1000}^1 =
(\Delta_{(1000,0)}^{1}) +
(\Delta_{(1000,500)}^{1}) +
(\Delta_{(1000,1500)}^{1}) +
(\Delta_{(1000,2000)}^{1})$$

$$density_{1000}^1 =
\frac{1}{1000} +
\frac{1}{500} +
\frac{1}{500} +
\frac{1}{1000} = 0.006$$

$$density_{1000}^2 = 
(\Delta_{(1000,0)}^{2}) +
(\Delta_{(1000,500)}^{2}) +
(\Delta_{(1000,1500)}^{2}) +
(\Delta_{(1000,2000)}^{2})$$

$$ density_{1000}^2 = 
(\frac{weight_{SSh}}{1000}) +
(\frac{weight_{SSh}}{500}) +
(\frac{weight_{SSh}}{500}) +
(\frac{weight_{SSh}}{1000}) =
\frac{3 * weight_{SSh}}{250} $$

$$ density_{1000}^3 =
\frac{3 * weight_{SSb}}{250} $$

$$ density_{1000} :=
{density_{1000}^1, density_{1000}^2, density_{1000}^3} =
\lbrace0.006,
\frac{3 * weight_{SSh}}{250},
\frac{3 * weight_{SSb}}{250}\rbrace $$

\subsection{Density Generalization}

In the case where we want to find $density_n$, where, n is the offset index, k is key count.

\[ 	
\begin{bmatrix}
	weight_{(n+\sigma,1)} & weight_{(n+\sigma,2)} & \dots  & weight_{(n+\sigma,k)} \\
	\vdots & \vdots & \udots & \vdots \\
	weight_{(n+1,1)} & weight_{(n+1,2)} & \dots  & weight_{(n+1,k)} \\
	weight_{(n,1)} & weight_{(n,2)} & \dots  & weight_{(n,k)} \\
    weight_{(n-1,1)} & weight_{(n-1,2)} & \dots  & weight_{(n-1,k)} \\
    \vdots & \vdots & \ddots & \vdots \\
    weight_{(n-\sigma,1)} & weight_{(n-\sigma,2)} & \dots  & weight_{(n-\sigma,k)}
\end{bmatrix}
\]
$$ * $$
\[
\begin{bmatrix}
	offset_{n+\sigma} & \dots & offset_{n+1} & offset_{n} & offset_{n-1} & \dots & offset_{n-\sigma} 
\end{bmatrix}
\]
$$ = $$
\[
\begin{bmatrix}
	density_{n+\sigma} & \dots & density_{n+1} & density_{n} & density_{n-1} & \dots & density_{n-\sigma} 
\end{bmatrix}
\]

$$
density_n :=
\begin{bmatrix}
	density_{n+\sigma} & \dots & density_{n+1} & density_{n} & density_{n-1} & \dots & density_{n-\sigma} 
\end{bmatrix}
$$ 

From here, we can calculate the strain by running the through a python code.
\subsection{Allocating Notes to Fingers}

We cannot assume that $column_1$ where $keys = 4$ is the same as $column_1$ where $keys = 7$. This is due to how \textbf{different fingers interact with the same column}.

As to counter this, we need to find out the \textbf{most common set-up} for players.

\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} 
	\hline
	key & LP & LR & LM & LI & S  & RI & RM & RR & RP\\
	\hline
	4   & {} & {} & 1  & 2  & {} & 3  & 4  & {} & {}\\
	5   & {} & {} & 1  & 2  & 3  & 4  & 5  & {} & {}\\
	6   & {} & 1  & 2  & 3  & {} & 4  & 5  & 6  & {}\\
	7   & {} & 1  & 2  & 3  & 4  & 5  & 6  & 7  & {}\\
	8   & 1  & 2  & 3  & 4  & {} & 5  & 6  & 7  & 8\\
	8S  & 1  & 2  & 3  & 4  & 5  & 6  & 7  & 8  & {}\\
	9   & 1  & 2  & 3  & 4  & 5  & 6  & 7  & 8  & 9\\
	\hline
\end{tabular}
\end{center}

\textbf{L} represents left, \textbf{R} is right, the second letter will be th	e name for the fingers (LP: Left Pinky, LR: Left Ring and so on...)

This allocation will give us a consistent result for all beatmaps, so $key=1$ will always mean \textbf{Left Pinky}, $key=2$ for \textbf{Left Ring}, and so on...

\subsubsection{8 Key Scratch Bias}
The issue with 8 Key maps is how maps will usually have a \textit{scratch column}, this will create a setup that \textbf{excludes a pinky but includes the thumb}. Due to this, we will shift the configuration to left pinky and right thumb, excluding the right pinky. See the above \textbf{8S}

\subsection{Assigning Hyperparameters}

In this section alone, we have used quite a few hyperparameters. To recap:

\paragraph{(Reading) $\theta$} is the hyperparameter for reading length.
\paragraph{(Density) $\sigma$} defines the range, front and back of the density search. Higher sigma may prove to be useless with further $\Delta_{nx}^k$ being too small.

\paragraph{(Density) $weight_{NN}$} defines for normal notes
\paragraph{(Density) $weight_{LNh}$} defines for long notes heads
\paragraph{(Density) $weight_{LNt}$} defines for long notes tails
\paragraph{(Density) $weight_{SSh}$} defines for \textbf{strain shift} for hands
\paragraph{(Density) $weight_{SSb}$} defines for \textbf{strain shift} for body 

Now what we need to do is to assign a reasonable values to these, and run the results to find our:
$$ difficulty := \lbrace(offset_1, difficulty_1), (offset_2, difficulty_2), ..., (offset_n, difficulty_n)\rbrace $$

Whereby we estimate difficulty at offset \textbf{n} from the map itself:

$$ difficulty_n \approx model \left( reading_n, \sum_{k=1}^{keys} \left(strain_k \right), ... \right)$$

We can further expand this to:

$$ difficulty_n \approx model \left( count(NN), count(LNh), count(LNt), \sum_{k=1}^{keys} \left(strain_k \right)  \right)  $$

Where strain

\subsubsection{Values of Hyperparameters}

There will definitely be issues when it comes to assuming hyperparameters because of how it will affect accuracy of the model. However, as long as we \textbf{reasonably} assign them, most of the errors will be offset by the neural network learning. We just need to focus on if a certain value should be \textbf{larger/smaller} or \textbf{negative/positive}.

\paragraph{(Reading) $\theta$} 1000 (ms)

\paragraph{(Density) $\sigma$} 2

\paragraph{(Density) $weight_{NN}$} 1
\paragraph{(Density) $weight_{LNh}$} 0.75 (we will follow Reading)
\paragraph{(Density) $weight_{LNt}$} 0.75 (we will follow Reading)
\paragraph{(Density) $weight_{SSh}$} 0.25
\paragraph{(Density) $weight_{SSb}$} 0.1

\end{document}
