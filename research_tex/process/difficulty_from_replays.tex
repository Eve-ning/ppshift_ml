\begin{document}
\section{Difficulty from Replays}

We are able to extract data from replays, however, they are not linked to the beatmaps themselves. This means that it only has data of what keys and when did the player press it, there's no data on accuracy achieved.

To recap, we managed to decode the replay sent into the following format:

$$ action_{replay} := \lbrace(offset_1, action_1), (offset_2, action_2), ... , (offset_n, action_n)\rbrace $$

Whereby, 
$$n \in \lbrace-9, -8, ... , -2, -1, 1, 2, ... , 8, 9\rbrace$$

$offset$ is when the $action$ happens. For $action$, $-n$ means the key \textbf{n} is released, $n$ means the key \textbf{n} is pressed. In this section, we will be discussing how we can make this a suitable output for the neural network to predict.

\subsection{Mapping a Replay to Beatmap}

In this, we match all similar actions in their respective columns.

There are a few things we need to take note of when matching:
\begin{enumerate}
	\item Not all $action_{beatmap}$ will have a matching $action_{replay}$
	\item We put the threshold of this matching as $100ms$, i.e. $action_{beatmap}$ that doesn't have any $action_{replay}$ within $100ms$ will be regarded as a miss.
	\item The nearest $action_{replay}$ will match the $action_{beatmap}$, not the earliest one.
	\item We will deviate on how osu! calculate accuracy due to the above pointers, this allows us to calculate on a more common basis.
\end{enumerate}

We will expect the output of:

$$ deviation := \lbrace(offset_1, deviation_1), (offset_2, deviation_2), ..., (offset_n, deviation_n)\rbrace $$

Where:
$$ n = length(action_{beatmap}) $$

And if there's no match, $deviation > 100$, this is to allow us to understand that it's a \textbf{miss} instead of a $100ms$ hit.

This doesn't proportionally represent accuracy (which will be easier to understand), as its lower value represents a better judgement, so we will adjust to the following:
$$ accuracy := \lbrace(offset_1, accuracy_1), (offset_2, accuracy_2), ..., (offset_n, accuracy_n)\rbrace $$

Where:
$$ accuracy_n = 100 - {deviation_n} $$

Therefore, a \textbf{miss} would simply just be $accuracy = 0$ instead.

So accuracy will only span:
$$ (Miss) 0 \leq accuracy_n \leq (Perfect) 1 $$
$$ where, deviation_n \in [0, 1, 2, ..., 99, 100] $$

\subsection{Replay Soloing}

As discussed in the preface, we need to resolve two issues. \textbf{Multiple Replays} and \textbf{Multiple Players}. We will expect an output similar to a replay, this is where the first major assumption kicks in.

\subsubsection{Assumption of the Top 50}
In this, we assume that if we took the \textbf{median} all top 50 replays, we will end up with a replay that is all-encompassing.

This leads some problems:

\paragraph{Top Player Bias} The neural network will perform worse on easier maps $ \approx 3.0 S.R. $ due to the median in easier maps being too consistently perfect, this leads to amplification of noise (in this case, chokes).
\paragraph{Population Interaction} This assumption will only hold if the beatmaps are old enough such that most of the general playerbase has played it, else it doesn't represent the population well enough due to low participation
\paragraph{Population Decay/Improvement} The beatmaps we check must be ranked within close proximity with each other, this is to avoid the Top 50 median from adjusting too much

\subsubsection{Assumption of the Player}
In this, instead of looking at it \textbf{per beatmap}, we will do it \textbf{per player}. This is much more consistent in data, however it's consistent \textbf{for that player only}.

In this method, we grab the all replays that \textbf{Player X} has played and compare them with each other. However this still leads to assumptions:

\begin{enumerate}
	\item The player played all the beatmaps at a similar time, or the player never improved
	\item The player is representative of the community (bias)
\end{enumerate}

The problem of the machine being \textbf{biased} to only the player can be fixed if we ran an averaging function on the output with hundreds of other players.

\textbf{However,} due to how osu! sends API scores, I couldn't get more than \textbf{50} scores \textbf{per player}. This means that there will be no data for these replays.

\subsection{Choosing an Assumption}

Despite this, we will work with \textbf{Assumption of the Top 50} as it's reasonable enough, and it's easier.

This means that the median of all replays will be saved in a different data file with $<beatmap\_id>.acrv$ extension. \textit{(v represents virtual)}

\subsection{Virtual Player/Replay}

The median of the top 50 creates a \textbf{virtual replay}, where we expect it to be a \textbf{good enough} representation of a \textbf{virtual player}. We can now pivot from this player as we calculate difficulty!

\subsection{Smoothing}

As expected, the median gathered will not be a smooth graph, we will smooth out with an \textbf{moving aggregation of its mean with a window of 30}. This means that it'll grab 30 data points, front and back, and calculate its median, creating a new series/vector.

As player's deviations usually will not be consistent throughout, this will help hammer down large errors, while also affecting its neighbouring data points. In turn, this aids the machine to learn that the error may not only be the issue of that one note, but instead a group of it.

\end{document}
