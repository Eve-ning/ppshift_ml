\documentclass{article}
\usepackage[left=2cm, right=5cm, top=2cm]{geometry}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{MnSymbol}
\begin{document}
	ppshift machine learning
	
	In this document, we will be discussing methods of obtaining a credible way of classifying difficulty in VSRG maps. We will first establish what makes a map difficult, then we build from there!
	
\part{Preface}
\section{Define Difficulty}

	What makes a map difficult, what is a difficult map? Could it be the following?
	The map was difficult because of ...
\begin{enumerate}
	\item Failing
	\item Combo Breaks
	\item High Stamina Requirement
	\item Low Accuracy
\end{enumerate}
We discuss all of these scenarios and we will choose one to tackle, possibly integrate the other options into our calculations in the future.

\paragraph{Failing}

The most significant way that we can readily control if players fail is via \textbf{Health Drain} in which most VSRGs will implement. However, this value is inconsistent and will not provide useful information on higher \textbf{Health Drain} values due to lack of players passing certain maps.

\paragraph{Combo Breaks}

Combo Breaks analysis is another method that isn't consistent, whereby chokes can be random, creating too much noise on higher skill plays. Combo breaks mainly can only determine the \textbf{hardest} points on the map, it doesn't depict a difficulty cure.

\paragraph{High Stamina Requirements}

While stamina is a good way to look at difficulty, it can readily be derived from accuracy, which is conveniently what we'll be looking at next

\paragraph{Low Accuracy}

This is the best way to look at difficulty, because not only it gives us a figure, it tells us the story and correlation between \textbf{accuracy} and \textbf{patterning}. This will be the main focus of the document.

\subsection{Comparing Accuracy in Replays to Patterns}

Details aside, how can we describe the impact of patterns on replays, what story does the replay tell us about the pattern?

Consider this...
\begin{enumerate}
	\item $Player_1$ plays $Pattern_A$ and $Pattern_B$
	\item $Player_1$ achieves $Accuracy_A > Accuracy_B$
	\item Considering $Accuracy_A$ and $Accuracy_B$ are independent events
	\item We deduce $Pattern_A < Pattern_B$ in difficulty
\end{enumerate}

It is a simple idea to pitch, but we will have to dive into more details on how we can "teach" a machine this concept and "learn" from it!

\section{Machine Learning}

A tangent from this article, I will briefly talk about what is machine learning.

\paragraph{Making a Prediction} Machine Learning is all about making a prediction, by looking at already curated data. \textit{We show the someone 100 pictures of rabbits and explained to them that they are rabbits, can they tell if the next picture of an animal is a rabbit?}

Same idea we have here, we show the machine hundred of beatmaps, we tell the machine how difficult all of them are (according to score regression). Can the machine predict the difficulty of the next one? The answer is yes, but it won't be accurate!

\subsection{Valid inputs}

Think about a neural network, you've most likely seen one, it's circle(s) connected to more circle(s), in the end, there will be circle(s) that tell you something. Each circle represents a \textbf{neuron}.

Each neuron holds a numeric value (between 0 and 1), we need to put a value in each of these neurons. Ignoring Timing Points and Scroll Speed Changes, how do we squeeze a beatmap into these neurons?

\paragraph{Hit Object Neurons} Remember that a neuron will only work with numeric values, what do you want to have in the neuron that represents a Hit Object? Putting either the \textbf{offset} or \textbf{column} will not work because both are vital!

\paragraph{Column Neurons} It is a possible idea, to have a neuron for \textbf{each millisecond}, then put \textbf{column value} on those neurons that match the \textbf{offset}. We run into a glaring issue where input neurons will stretch to the \textbf{hundred thousands}, computational power required on this scale would be too high. If we decide to \textbf{bin} the offsets (binning is the act of grouping things together to reduce the number of fields) to \textbf{100ms} steps, it'll still be in the thousands.

\subsection{Subdivide the Issue}

\textbf{Does it have to be the whole map?}

It doesn't! That's what we are doing for this research, we don't look at the whole map, we look at parts of it, the inner mechanisms, the \textbf{patterns}! Sure, even though the whole map must be present to calculate difficulty but if we break it into \textbf{2 distinct procedures}, it's easier to visualize and control the system.

\begin{enumerate}
	\item Patterns to Difficulties
	\item Difficulties to  Map Rating
\end{enumerate}

\paragraph{Patterns to Difficulties} In the following sections we will be discussing how we can task the machine to learn what patterns are easy and which are harder

\paragraph{Difficulties to Map Rating} Using the \textbf{difficulty} values defined previously, we can find out how to use that to reliably give the map a rating. Do we rate it by the average, maximum or median? We will discuss those later.

\part{Data Analysis}

\subsection{Replay data to actions}

In order to compare $accuracy$ and $patterning$ we need to change our perspective, instead of looking at $accuracy$ as a number, we will look at it as a vector. Whereby we extract player data from provided replays.

\subsubsection{GETting beatmap\_id data from osu!API}

Using osu!API, I was able to extract all \textbf{beatmap IDs} where $ Star Rating \leq 3.0 $ as any maps below this $Star Rating$ will usually only have \textbf{SS} scores, which will prove to be redundant in analysis.

With this list, we are able to know what maps we are tackling in the following sections.

\subsubsection{GETting Replay data from osu!API}

osu!API provides us with essential data, including the replay file itself. Not going into detail, we are able to extract all key taps (including releases) of the player during the play.

I will provide python code I have used in the GitHub Repository or in the Annex of the document.

The format we get from running the python code goes as follows

$$ action_{replay} := \lbrace(offset_1, action_1), (offset_2, action_2), ... , (offset_n, action_n)\rbrace $$

Whereby, 
$$n \in \lbrace-9, -8, ... , -2, -1, 1, 2, ... , 8, 9\rbrace$$

$offset$ is when the $action$ happens. For $action$, $-n$ means the key \textbf{n} is released, $n$ means the key \textbf{n} is pressed.

We will save this data in a file with $<beatmap\_id>.acr$ extension.

\subsection{Difficulty data to actions}

In order to make use of $Action_{replay}$, we need to obtain $Action_{difficulty}$.

\subsubsection{Downloading Difficulties through web crawling}

Using python, we can download $.osu$ (osu! difficulty file extension) using the format below with authentication:

$$https://osu.ppy.sh/osu/<beatmap\_id>$$

We save all of these using $<beatmap\_id>.osu$ file naming system.

\subsubsection{Converting Difficulties to Action format}

As to be aligned with $action_{replay}$ format, we need to convert all difficulties to $action_{difficulty}$. This can be done with a python script.

However, we should note that maps with \textbf{variable scroll speeds} will cause anomalies in our calculation, so we need to further define what maps defy a threshold we set to remove these from our calculations.

\paragraph{Scroll Speed Manipulation Threshold}
A simple way of tackling this issue would be to skip all beatmaps that has any of the following:
\begin{enumerate}
	\item Any \textbf{slight} SV Change $ 0.97 \leq TP_{SV} \leq 1.03 $
	\item Any BPM Change
\end{enumerate}

After skipping beatmaps, we will grab all remaining $.osu$ and convert them to $.acd$. The action format is the same idea, where we have a list of offset and action.

\subsection{Mapping $action_{replay}$ to $action_{difficulty}$}

This is the last essential step to find out $accuracy$ as a vector. We will match all similar actions in their respective columns together.

There are a few things we need to take note of when matching:
\begin{enumerate}
	\item Not all $action_{difficulty}$ will have a matching $action_{replay}$
	\item We put the threshold of this matching as $100ms$.
	\item The nearest $action_{replay}$ will match the $action_{difficulty}$, not the earliest one.
	\item We will deviate on how osu! calculate accuracy due to the above pointers, but its difference is insignificant.
\end{enumerate}

We will expect the output of:

$$ deviation := \lbrace(offset_1, deviation_1), (offset_2, deviation_2), ..., (offset_n, deviation_n)\rbrace $$

Where:
$$ n = length(action_{difficulty}) $$

And if there's no match, $deviation = 101$, this is to allow us to understand that it's a \textbf{miss} instead of a $100ms$ hit.

However, this is a bit ugly, so what we will use is the following:
$$ accuracy := \lbrace(offset_1, accuracy_1), (offset_2, accuracy_2), ..., (offset_n, accuracy_n)\rbrace $$

Where:
$$ accuracy_n = \frac{1}{deviation_n} $$
$$ n = length(action_{difficulty}) $$

Therefore, a \textbf{miss} would simply just be $accuracy = 0$ instead of the ugly $accuracy = 1/101$

So accuracy will only span:
$$ (Miss) 0 \leq accuracy_n \leq (Perfect) 1 $$
$$ deviation_n \in [0, 1, 2, ..., 99, 100] $$

\section{Difficulty from Patterning}

We turn our attention to how we can figure out difficulty from the map itself, the expected output we want would be:

$$ difficulty := \lbrace(offset_1, difficulty_1), (offset_2, difficulty_2), ..., (offset_n, difficulty_n)\rbrace $$

Whereby we estimate difficulty at offset \textbf{n} from the map itself:

$$ difficulty_n \approx reading_n + \sum_{k=1}^{keys} \left(strain_k \right) + ... $$

There are more factors (denoted by $...$) that contribute to difficulty, but we will regard them as noise in this research and fine tune this equation later.

\paragraph{Reading} This denotes how hard is it to read all the patterns on the screen. We can draw similarities between this and density, however this looks beyond note density and estimates the difficulty of reading different similar density patterns.

\paragraph{Strain} This is reliant on $density$ whereby continuous high values of $ density$ will result in a high $strain$. This has an additional hyperparameter, $decay$, where it denotes how fast the player can recover from $strain_n$. Finger $strain$ on the same hand will likely affect the other $strain$ values of the other fingers.

\paragraph{Density} This focuses on the imminent density of the offset (contrary to strain), whereby it disregards the global trends of patterns.

\subsection{Note Type Weights}

This will define the \textbf{weightages} of each note type.
\paragraph{$weight_{NN}$} defines for normal notes
\paragraph{$weight_{LNh}$} defines for long notes heads
\paragraph{$weight_{LNt}$} defines for long notes tails
\paragraph{$weight_{SSh}$} defines for \textbf{strain shift} for hands \textbf{(explained later)}
\paragraph{$weight_{SSb}$} defines for \textbf{strain shift} for body \textbf{(explained later)}

\subsection{Reading}

$$ reading_{(n,n+\theta)} = 
\frac{ count(NN) + \left[ count(LNh) + count(LNt)\right] * \Gamma }{\theta}
= \lbrace n \leq offset \leq (n+ \theta) \rbrace$$

Where,

\paragraph{$n$} is the initial offset
\paragraph{$\theta$} is the hyperparameter for length.
\paragraph{$\Gamma$} is the hyperparameter for how difficult a long note is to read

We will not take into consideration the length of $note_{long}$

\subsection{Density}

We will look into density before strain as it's derived from this.

Considering the notes on the $k$ column
$$ \lbrace n-2, n-1, n, n+1, n+2 \rbrace $$
$$ \Delta_{nx}^k = \frac{1}{n - x}$$
$$ density_n^k =
\sum_{N=n-\sigma}^{n+\sigma}
\left(
\Delta_{nN}^k
\right)$$

So for $\sigma = 2$ and 
$$ column_k := \lbrace a, b, n, d, e\rbrace$$
$$ density_n^k = \Delta_{na}^k + \Delta_{nb}^k + \Delta_{nd}^k + \Delta_{ne}^k $$

\paragraph{$\Delta_{nx}^k$} will be the the inverse of the (ms) distance between notes $n$ and $x$ on column $k$. Notes that are further away will be penalized with a square. 

\paragraph{$\sigma$} defines the range, front and back of the search. Higher sigma may prove to be useless with further $\Delta_{nx}^k$ being too small.


\subsection{Strain}

This will work in relationship with $density$, whereby a $strain$ is a cumulative function of $density$ with a \textbf{linear decay function}.

Notes:
\begin{enumerate}
	\item Better players have \textbf{higher decay gradients}
	\item If $decay > density$, $strain$ will \textbf{decrease}
	\item If $decay < density$, $strain$ will \textbf{increase}
	\item There will be a point where $strain$ is high enough to affect physical performance, indirectly affecting accuracy.
\end{enumerate}

\subsubsection{Strain Shift}

Strain will not only affect one finger, it will affect the hand and both after time, just on a smaller scale

\paragraph{Hand} We will denote the strain shift hyperparameter of one finger to another on the same hand to be $SS_H$
\paragraph{Body} Likewise, for body, we will denote as $SS_B$

\subsubsection{Strain Example}

Consider the case, without \textbf{Strain Shift}
$$ Where, weight_{NN} = 1, \sigma = 2 $$
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|} 
	\hline
	2500 & 0 			& 0 & 0 &       & 0.022 & 0.016\\ \hline
	2000 & $weight_{NN}$& 0 & 0 & 0.003 & 0.022 & 0.017\\	\hline
	1500 & $weight_{NN}$& 0 & 0 & 0.005 & 0.019 & 0.015\\	\hline
	1000 & $weight_{NN}$& 0 & 0 & 0.006 & 0.014 & 0.011\\	\hline
	 500 & $weight_{NN}$& 0 & 0 & 0.005 & 0.008 & 0.006\\	\hline
	   0 & $weight_{NN}$& 0 & 0 & 0.003 & 0.003 & 0.002\\	\hline
    -500 & 0 			& 0 & 0 & 		& 0	 	& 0	\\	\hline
   -1000 & 0 			& 0 & 0 & 		& 0	 	& 0	\\
	\hline
	Offset(ms) & k=1 & k=2 & k=3 & $\approx Density$ & Strain (dec=0) & Strain (dec=0.001) \\ 
	\hline
\end{tabular}
\end{center}

Consider the case, with \textbf{Strain Shift}
\begin{center}
	\begin{tabular}{|c|c|c|c|} 
	\hline
	2500 & 0			  & 0 		& 0 	\\ \hline
	2000 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	1500 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	1000 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	 500 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	   0 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
    -500 & 0			  & 0 		& 0 	\\	\hline
   -1000 & 0			  & 0 		& 0 	\\
	\hline
	Offset(ms) & k=1 & k=2 & k=3\\ 
	\hline
\end{tabular}
\end{center}

It's hard to include the calculations in the table, so we'll look at $density_{(1,1000)}$, we will also elaborate on the calculations without strain shift.

$$density_{1000}^1 =
(\Delta_{(1000,0)}^{1}) +
(\Delta_{(1000,500)}^{1}) +
(\Delta_{(1000,1500)}^{1}) +
(\Delta_{(1000,2000)}^{1})$$

$$density_{1000}^1 =
\frac{1}{1000} +
\frac{1}{500} +
\frac{1}{500} +
\frac{1}{1000} = 0.006$$

$$density_{1000}^2 = 
(\Delta_{(1000,0)}^{2}) +
(\Delta_{(1000,500)}^{2}) +
(\Delta_{(1000,1500)}^{2}) +
(\Delta_{(1000,2000)}^{2})$$

$$ density_{1000}^2 = 
(\frac{weight_{SSh}}{1000}) +
(\frac{weight_{SSh}}{500}) +
(\frac{weight_{SSh}}{500}) +
(\frac{weight_{SSh}}{1000}) =
\frac{3 * weight_{SSh}}{250} $$

$$ density_{1000}^3 =
\frac{3 * weight_{SSb}}{250} $$

$$ density_{1000} =
density_{1000}^1 + density_{1000}^2 + density_{1000}^3 =
0.006 +
\frac{3 * weight_{SSh}}{250} + \frac{3 * weight_{SSb}}{250} $$

\subsection{Density Generalization}

In the case where we want to find $density_n$, where, n is the offset index, k is key count.

\[ 	
\begin{bmatrix}
	weight_{(n+\sigma,1)} & weight_{(n+\sigma,2)} & \dots  & weight_{(n+\sigma,k)} \\
	\vdots & \vdots & \udots & \vdots \\
	weight_{(n+1,1)} & weight_{(n+1,2)} & \dots  & weight_{(n+1,k)} \\
	weight_{(n,1)} & weight_{(n,2)} & \dots  & weight_{(n,k)} \\
    weight_{(n-1,1)} & weight_{(n-1,2)} & \dots  & weight_{(n-1,k)} \\
    \vdots & \vdots & \ddots & \vdots \\
    weight_{(n-\sigma,1)} & weight_{(n-\sigma,2)} & \dots  & weight_{(n-\sigma,k)}
\end{bmatrix}
\]
$$ * $$
\[
\begin{bmatrix}
	offset_{n+\sigma} & \dots & offset_{n+1} & offset_{n} & offset_{n-1} & \dots & offset_{n-\sigma} 
\end{bmatrix}
\]
$$ = $$
\[
\begin{bmatrix}
	density_{n+\sigma} & \dots & density_{n+1} & density_{n} & density_{n-1} & \dots & density_{n-\sigma} 
\end{bmatrix}
\]

$$ \sum
\begin{bmatrix}
	density_{n+\sigma} & \dots & density_{n+1} & density_{n} & density_{n-1} & \dots & density_{n-\sigma} 
\end{bmatrix}
= density_n
$$ 

From here, we can calculate the strain by running the through a python code.

\subsection{Assigning Hyperparameters}

In this section alone, we have used quite a few hyperparameters. To recap:

\paragraph{(Reading) $\theta$} is the hyperparameter for reading length.
\paragraph{(Reading) $\Gamma$} is the hyperparameter for how difficult a long note is to read

\paragraph{(Density) $\sigma$} defines the range, front and back of the density search. Higher sigma may prove to be useless with further $\Delta_{nx}^k$ being too small.

\paragraph{(Density) $weight_{NN}$} defines for normal notes
\paragraph{(Density) $weight_{LNh}$} defines for long notes heads
\paragraph{(Density) $weight_{LNt}$} defines for long notes tails
\paragraph{(Density) $weight_{SSh}$} defines for \textbf{strain shift} for hands
\paragraph{(Density) $weight_{SSb}$} defines for \textbf{strain shift} for body 

Now what we need to do is to assign a reasonable values to these, and run the results to find our:
$$ difficulty := \lbrace(offset_1, difficulty_1), (offset_2, difficulty_2), ..., (offset_n, difficulty_n)\rbrace $$

Whereby we estimate difficulty at offset \textbf{n} from the map itself:

$$ difficulty_n \approx reading_n + \sum_{k=1}^{keys} \left(strain_k \right) $$

\section{Creating a Neural Network}

Before we dive into creating a neural network to finalize the calculation, we need to define specifically these few things:

\begin{enumerate}
	\item Input
	\item Layers
	\item Output
\end{enumerate}

\subsection{What is our input?}

Our input will be simply the $difficulty$ (matrix/2D vector).



\end{document}
