\documentclass{article}
\usepackage[left=2cm, right=5cm, top=2cm]{geometry}
\usepackage{listings}
\begin{document}
	ppshift machine learning
	
	In this document, we will be discussing methods of obtaining a credible way of classifying difficulty in VSRG maps. We will first establish what makes a map difficult, then we build from there!
	
\part{Define Difficulty}
\section{Difficulty in Playing}

	What makes a map difficult, what is a difficult map? Could it be the following?
	The map was difficult because of ...
\begin{enumerate}
	\item Failing
	\item Combo Breaks
	\item High Stamina Requirement
	\item Low Accuracy
\end{enumerate}
We discuss all of these scenarios and we will choose one to tackle, possibly integrate the other options into our calculations in the future.

\paragraph{Failing}

The most significant way that we can readily control if players fail is via \textbf{Health Drain} in which most VSRGs will implement. However, this value is inconsistent and will not provide useful information on higher \textbf{Health Drain} values due to lack of players passing certain maps.

\paragraph{Combo Breaks}

Combo Breaks analysis is another method that isn't consistent, whereby chokes can be random, creating too much noise on higher skill plays. Combo breaks mainly can only determine the \textbf{hardest} points on the map, it doesn't depict a difficulty cure.

\paragraph{High Stamina Requirements}

While stamina is a good way to look at difficulty, it can readily be derived from accuracy, which is conveniently what we'll be looking at next

\paragraph{Low Accuracy}

This is the best way to look at difficulty, because not only it gives us a figure, it tells us the story and correlation between \textbf{accuracy} and \textbf{patterning}. This will be the main focus of the document.

\section{Difficulty from Accuracy}

It is possible to measure difficulty, by just looking at accuracy, in-fact, it's quite straight-forward to do so.

Consider this...
\begin{enumerate}
	\item $Player_1$ plays $Beatmap_A$ and $Beatmap_B$
	\item $Player_1$ achieves $Accuracy_A > Accuracy_B$
	\item Considering $Accuracy_A$ and $Accuracy_B$ are independent events
	\item We deduce $Difficulty_A < Difficulty_B$
\end{enumerate}

Now, on a larger scale...
\begin{enumerate}
	\item $Player_{all}$ play $Beatmap_A$ and $Beatmap_B$
	\item $\prod_{} (Accuracy_A / Accuracy_B) > 1$
	\item Considering $Accuracy_A$ and $Accuracy_B$ are independent events
	\item We deduce $Difficulty_A < Difficulty_B$
\end{enumerate}

We can further develop this algorithm to estimate what the difficulties would be by just looking at the ratios, I won't elaborate this further as this won't be the main aim of this document because \textbf{this only works if you have scores to pivot from}. This will not work with newly developed maps.

\subsection{Replay data to accuracies}

In order to compare $A$ (Accuracy) and $P$ (Patterning) we need to change our perspective, instead of looking at $A$ as a number, we will look at it as a vector. Whereby we extract player data from provided replays.

\subsubsection{GETting beatmap\_id data from osu!API}

Using osu!API, I was able to extract all \textbf{beatmap IDs} where $ Star Rating \leq 3.0 $ as any maps below this $Star Rating$ will usually only have \textbf{SS} scores, which will prove to be redundant in analysis.

With this list, we are able to know what maps we are tackling in the following sections.

\subsubsection{GETting Replay data from osu!API}

osu!API provides us with essential data, including the replay file itself. Not going into detail, we are able to extract all key taps (including releases) of the player during the play.

I will provide python code I have used in the GitHub Repository or in the Annex of the document.

The format we get from running the python code goes as follows

$$ action_{replay} = \lbrace(offset_1, action_1), (offset_2, action_2), ... , (offset_n, action_n)\rbrace $$

Whereby, 
$$n \in \lbrace-9, -8, ... , -2, -1, 1, 2, ... , 8, 9\rbrace$$

$offset$ is when the $action$ happens. For $action$, $-n$ means the key \textbf{n} is released, $n$ means the key \textbf{n} is pressed.

We will save this data in a file with $<beatmap\_id>.acr$ extension.

\subsubsection{Downloading Difficulties from web crawling}

In order to make use of $Action_{replay}$, we need to obtain $Action_{difficulty}$. Using python, we can download $.osu$ (osu! difficulty file extension) using the format below with authentication:

$$https://osu.ppy.sh/osu/<beatmap\_id>$$

We save all of these using $<beatmap\_id>.osu$ file naming system.

\subsubsection{Converting Difficulties to Action format}

As to be aligned with $action_{replay}$ format, we need to convert all difficulties to $action_{difficulty}$. This can be done with a python script.

However, we should note that maps with \textbf{variable scroll speeds} will cause anomalies in our calculation, so we need to further define what maps defy a threshold we set to remove these from our calculations.

\paragraph{An SV Threshold}
A simple way of tackling this issue would be to skip all beatmaps that has any of the following:
\begin{enumerate}
	\item Any \textbf{slight} SV Change $ 0.97 \leq TP_{SV} \leq 1.03 $
	\item Any BPM Change
\end{enumerate}

\section{Difficulty in Mapping}






\end{document}