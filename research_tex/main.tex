\documentclass{article}
\usepackage[left=2cm, right=5cm, top=2cm]{geometry}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{MnSymbol}
\begin{document}
	ppshift machine learning
	
	In this document, we will be discussing methods of obtaining a credible way of classifying difficulty in VSRG maps. We will first establish what makes a map difficult, then we build from there!
	
\part{Define Difficulty}
\section{Difficulty in Playing}

	What makes a map difficult, what is a difficult map? Could it be the following?
	The map was difficult because of ...
\begin{enumerate}
	\item Failing
	\item Combo Breaks
	\item High Stamina Requirement
	\item Low Accuracy
\end{enumerate}
We discuss all of these scenarios and we will choose one to tackle, possibly integrate the other options into our calculations in the future.

\paragraph{Failing}

The most significant way that we can readily control if players fail is via \textbf{Health Drain} in which most VSRGs will implement. However, this value is inconsistent and will not provide useful information on higher \textbf{Health Drain} values due to lack of players passing certain maps.

\paragraph{Combo Breaks}

Combo Breaks analysis is another method that isn't consistent, whereby chokes can be random, creating too much noise on higher skill plays. Combo breaks mainly can only determine the \textbf{hardest} points on the map, it doesn't depict a difficulty cure.

\paragraph{High Stamina Requirements}

While stamina is a good way to look at difficulty, it can readily be derived from accuracy, which is conveniently what we'll be looking at next

\paragraph{Low Accuracy}

This is the best way to look at difficulty, because not only it gives us a figure, it tells us the story and correlation between \textbf{accuracy} and \textbf{patterning}. This will be the main focus of the document.

\section{Difficulty from Accuracy}

It is possible to measure difficulty, by just looking at accuracy, in-fact, it's quite straight-forward to do so.

Consider this...
\begin{enumerate}
	\item $Player_1$ plays $Beatmap_A$ and $Beatmap_B$
	\item $Player_1$ achieves $Accuracy_A > Accuracy_B$
	\item Considering $Accuracy_A$ and $Accuracy_B$ are independent events
	\item We deduce $Difficulty_A < Difficulty_B$
\end{enumerate}

Now, on a larger scale...
\begin{enumerate}
	\item $Player_{all}$ play $Beatmap_A$ and $Beatmap_B$
	\item $\prod_{} (Accuracy_A / Accuracy_B) > 1$
	\item Considering $Accuracy_A$ and $Accuracy_B$ are independent events
	\item We deduce $Difficulty_A < Difficulty_B$
\end{enumerate}

We can further develop this algorithm to estimate what the difficulties would be by just looking at the ratios, I won't elaborate this further as this won't be the main aim of this document because \textbf{this only works if you have scores to pivot from}. This will not work with newly developed maps.

\subsection{Replay data to actions}

In order to compare $accuracy$ and $patterning$ we need to change our perspective, instead of looking at $accuracy$ as a number, we will look at it as a vector. Whereby we extract player data from provided replays.

\subsubsection{GETting beatmap\_id data from osu!API}

Using osu!API, I was able to extract all \textbf{beatmap IDs} where $ Star Rating \leq 3.0 $ as any maps below this $Star Rating$ will usually only have \textbf{SS} scores, which will prove to be redundant in analysis.

With this list, we are able to know what maps we are tackling in the following sections.

\subsubsection{GETting Replay data from osu!API}

osu!API provides us with essential data, including the replay file itself. Not going into detail, we are able to extract all key taps (including releases) of the player during the play.

I will provide python code I have used in the GitHub Repository or in the Annex of the document.

The format we get from running the python code goes as follows

$$ action_{replay} := \lbrace(offset_1, action_1), (offset_2, action_2), ... , (offset_n, action_n)\rbrace $$

Whereby, 
$$n \in \lbrace-9, -8, ... , -2, -1, 1, 2, ... , 8, 9\rbrace$$

$offset$ is when the $action$ happens. For $action$, $-n$ means the key \textbf{n} is released, $n$ means the key \textbf{n} is pressed.

We will save this data in a file with $<beatmap\_id>.acr$ extension.

\subsection{Difficulty data to actions}

In order to make use of $Action_{replay}$, we need to obtain $Action_{difficulty}$.

\subsubsection{Downloading Difficulties through web crawling}

Using python, we can download $.osu$ (osu! difficulty file extension) using the format below with authentication:

$$https://osu.ppy.sh/osu/<beatmap\_id>$$

We save all of these using $<beatmap\_id>.osu$ file naming system.

\subsubsection{Converting Difficulties to Action format}

As to be aligned with $action_{replay}$ format, we need to convert all difficulties to $action_{difficulty}$. This can be done with a python script.

However, we should note that maps with \textbf{variable scroll speeds} will cause anomalies in our calculation, so we need to further define what maps defy a threshold we set to remove these from our calculations.

\paragraph{Scroll Speed Manipulation Threshold}
A simple way of tackling this issue would be to skip all beatmaps that has any of the following:
\begin{enumerate}
	\item Any \textbf{slight} SV Change $ 0.97 \leq TP_{SV} \leq 1.03 $
	\item Any BPM Change
\end{enumerate}

After skipping beatmaps, we will grab all remaining $.osu$ and convert them to $.acd$. The action format is the same idea, where we have a list of offset and action.

\subsection{Mapping $action_{replay}$ to $action_{difficulty}$}

This is the last essential step to find out $accuracy$ as a vector. We will match all similar actions in their respective columns together.

There are a few things we need to take note of when matching:
\begin{enumerate}
	\item Not all $action_{difficulty}$ will have a matching $action_{replay}$
	\item We put the threshold of this matching as $100ms$.
	\item The nearest $action_{replay}$ will match the $action_{difficulty}$, not the earliest one.
	\item We will deviate on how osu! calculate accuracy due to the above pointers, but its difference is insignificant.
\end{enumerate}

We will expect the output of:

$$ deviation := \lbrace(offset_1, deviation_1), (offset_2, deviation_2), ..., (offset_n, deviation_n)\rbrace $$

Where:
$$ n = length(action_{difficulty}) $$

And if there's no match, $deviation = 101$, this is to allow us to understand that it's a \textbf{miss} instead of a $100ms$ hit.

However, this is a bit ugly, so what we will use is the following:
$$ accuracy := \lbrace(offset_1, accuracy_1), (offset_2, accuracy_2), ..., (offset_n, accuracy_n)\rbrace $$

Where:
$$ accuracy_n = \frac{1}{deviation_n} $$
$$ n = length(action_{difficulty}) $$

Therefore, a \textbf{miss} would simply just be $accuracy = 0$ instead of the ugly $accuracy = 1/101$

So accuracy will only span:
$$ (Miss) 0 \leq accuracy_n \leq (Perfect) 1 $$
$$ deviation_n \in [0, 1, 2, ..., 99, 100] $$

\section{Difficulty from Patterning}

We turn our attention to how we can figure out difficulty from the map itself, the expected output we want would be:

$$ difficulty := \lbrace(offset_1, difficulty_1), (offset_2, difficulty_2), ..., (offset_n, difficulty_n)\rbrace $$

Whereby we estimate difficulty at offset \textbf{n} from the map itself:

$$ difficulty_n \approx reading_n + \sum_{k=1}^{keys} \left(strain_k \right) + ... $$

There are more factors (denoted by $...$) that contribute to difficulty, but we will regard them as noise in this research and fine tune this equation later.

\paragraph{Reading} This denotes how hard is it to read all the patterns on the screen. We can draw similarities between this and density, however this looks beyond note density and estimates the difficulty of reading different similar density patterns.

\paragraph{Strain} This is reliant on $density$ whereby continuous high values of $ density$ will result in a high $strain$. This has an additional hyperparameter, $decay$, where it denotes how fast the player can recover from $strain_n$. Finger $strain$ on the same hand will likely affect the other $strain$ values of the other fingers.

\paragraph{Density} This focuses on the imminent density of the offset (contrary to strain), whereby it disregards the global trends of patterns.

\subsection{Note Type Weights}

This will define the \textbf{weightages} of each note type.
\paragraph{$weight_{NN}$} defines for normal notes
\paragraph{$weight_{LNh}$} defines for long notes heads
\paragraph{$weight_{LNt}$} defines for long notes tails
\paragraph{$weight_{SSh}$} defines for \textbf{strain shift} for hands \textbf{(explained later)}
\paragraph{$weight_{SSb}$} defines for \textbf{strain shift} for body \textbf{(explained later)}

\subsection{Reading}

$$ reading_{(n,n+\theta)} = 
\frac{ count(NN) + \left[ count(LNh) + count(LNt)\right] * \Gamma }{\theta}
= \lbrace n \leq offset \leq (n+ \theta) \rbrace$$

Where,

\paragraph{$n$} is the initial offset
\paragraph{$\theta$} is the hyperparameter for length.
\paragraph{$\Gamma$} is the hyperparameter for how difficult a long note is to read

We will not take into consideration the length of $note_{long}$

\subsection{Density}

We will look into density before strain as it's derived from this.

Considering the notes on the $k$ column
$$ \lbrace n-2, n-1, n, n+1, n+2 \rbrace $$
$$ \Delta_{nx}^k = \frac{1}{n - x}$$
$$ density_n^k =
\sum_{N=n-\sigma}^{n+\sigma}
\left(
\Delta_{nN}^k
\right)$$

So for $\sigma = 2$ and 
$$ column_k := \lbrace a, b, n, d, e\rbrace$$
$$ density_n^k = \Delta_{na}^k + \Delta_{nb}^k + \Delta_{nd}^k + \Delta_{ne}^k $$

\paragraph{$\Delta_{nx}^k$} will be the the inverse of the (ms) distance between notes $n$ and $x$ on column $k$. Notes that are further away will be penalized with a square. 

\paragraph{$\sigma$} defines the range, front and back of the search. Higher sigma may prove to be useless with further $\Delta_{nx}^k$ being too small.


\subsection{Strain}

This will work in relationship with $density$, whereby a $strain$ is a cumulative function of $density$ with a \textbf{linear decay function}.

Notes:
\begin{enumerate}
	\item Better players have \textbf{higher decay gradients}
	\item If $decay > density$, $strain$ will \textbf{decrease}
	\item If $decay < density$, $strain$ will \textbf{increase}
	\item There will be a point where $strain$ is high enough to affect physical performance, indirectly affecting accuracy.
\end{enumerate}

\subsubsection{Strain Shift}

Strain will not only affect one finger, it will affect the hand and both after time, just on a smaller scale

\paragraph{Hand} We will denote the strain shift hyperparameter of one finger to another on the same hand to be $SS_H$
\paragraph{Body} Likewise, for body, we will denote as $SS_B$

\subsubsection{Strain Example}

Consider the case, without \textbf{Strain Shift}
$$ Where, weight_{NN} = 1, \sigma = 2 $$
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|} 
	\hline
	2500 & 0 			& 0 & 0 &       & 0.022 & 0.016\\ \hline
	2000 & $weight_{NN}$& 0 & 0 & 0.003 & 0.022 & 0.017\\	\hline
	1500 & $weight_{NN}$& 0 & 0 & 0.005 & 0.019 & 0.015\\	\hline
	1000 & $weight_{NN}$& 0 & 0 & 0.006 & 0.014 & 0.011\\	\hline
	 500 & $weight_{NN}$& 0 & 0 & 0.005 & 0.008 & 0.006\\	\hline
	   0 & $weight_{NN}$& 0 & 0 & 0.003 & 0.003 & 0.002\\	\hline
    -500 & 0 			& 0 & 0 & 		& 0	 	& 0	\\	\hline
   -1000 & 0 			& 0 & 0 & 		& 0	 	& 0	\\
	\hline
	Offset(ms) & k=1 & k=2 & k=3 & $\approx Density$ & Strain (dec=0) & Strain (dec=0.001) \\ 
	\hline
\end{tabular}
\end{center}

Consider the case, with \textbf{Strain Shift}
\begin{center}
	\begin{tabular}{|c|c|c|c|} 
	\hline
	2500 & 0			  & 0 		& 0 	\\ \hline
	2000 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	1500 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	1000 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	 500 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
	   0 & $weight_{NN}$  & $weight_{SSh}$ 	& $weight_{SSb}$\\	\hline
    -500 & 0			  & 0 		& 0 	\\	\hline
   -1000 & 0			  & 0 		& 0 	\\
	\hline
	Offset(ms) & k=1 & k=2 & k=3\\ 
	\hline
\end{tabular}
\end{center}

It's hard to include the calculations in the table, so we'll look at $density_{(1,1000)}$, we will also elaborate on the calculations without strain shift.

$$density_{1000}^1 =
(\Delta_{(1000,0)}^{1}) +
(\Delta_{(1000,500)}^{1}) +
(\Delta_{(1000,1500)}^{1}) +
(\Delta_{(1000,2000)}^{1})$$

$$density_{1000}^1 =
\frac{1}{1000} +
\frac{1}{500} +
\frac{1}{500} +
\frac{1}{1000} = 0.006$$

$$density_{1000}^2 = 
(\Delta_{(1000,0)}^{2}) +
(\Delta_{(1000,500)}^{2}) +
(\Delta_{(1000,1500)}^{2}) +
(\Delta_{(1000,2000)}^{2})$$

$$ density_{1000}^2 = 
(\frac{weight_{SSh}}{1000}) +
(\frac{weight_{SSh}}{500}) +
(\frac{weight_{SSh}}{500}) +
(\frac{weight_{SSh}}{1000}) =
\frac{3 * weight_{SSh}}{250} $$

$$ density_{1000}^3 =
\frac{3 * weight_{SSb}}{250} $$

$$ density_{1000} =
density_{1000}^1 + density_{1000}^2 + density_{1000}^3 =
0.006 +
\frac{3 * weight_{SSh}}{250} + \frac{3 * weight_{SSb}}{250} $$

\subsection{Density Generalization}

In the case where we want to find $density_n$, where, n is the offset index, k is key count.

\[ 	
\begin{bmatrix}
	weight_{(n+\sigma,1)} & weight_{(n+\sigma,2)} & \dots  & weight_{(n+\sigma,k)} \\
	\vdots & \vdots & \udots & \vdots \\
	weight_{(n+1,1)} & weight_{(n+1,2)} & \dots  & weight_{(n+1,k)} \\
	weight_{(n,1)} & weight_{(n,2)} & \dots  & weight_{(n,k)} \\
    weight_{(n-1,1)} & weight_{(n-1,2)} & \dots  & weight_{(n-1,k)} \\
    \vdots & \vdots & \ddots & \vdots \\
    weight_{(n-\sigma,1)} & weight_{(n-\sigma,2)} & \dots  & weight_{(n-\sigma,k)}
\end{bmatrix}
\]
$$ * $$
\[
\begin{bmatrix}
	offset_{n+\sigma} & \dots & offset_{n+1} & offset_{n} & offset_{n-1} & \dots & offset_{n-\sigma} 
\end{bmatrix}
\]
$$ = $$
\[
\begin{bmatrix}
	density_{n+\sigma} & \dots & density_{n+1} & density_{n} & density_{n-1} & \dots & density_{n-\sigma} 
\end{bmatrix}
\]

$$ \sum
\begin{bmatrix}
	density_{n+\sigma} & \dots & density_{n+1} & density_{n} & density_{n-1} & \dots & density_{n-\sigma} 
\end{bmatrix}
= density_n
$$ 

From here, we can calculate the strain by running the through a python code.

\subsection{Assigning Hyperparameters}

In this section alone, we have used quite a few hyperparameters. To recap:

\paragraph{(Reading) $\theta$} is the hyperparameter for reading length.
\paragraph{(Reading) $\Gamma$} is the hyperparameter for how difficult a long note is to read

\paragraph{(Density) $\sigma$} defines the range, front and back of the density search. Higher sigma may prove to be useless with further $\Delta_{nx}^k$ being too small.

\paragraph{(Density) $weight_{NN}$} defines for normal notes
\paragraph{(Density) $weight_{LNh}$} defines for long notes heads
\paragraph{(Density) $weight_{LNt}$} defines for long notes tails
\paragraph{(Density) $weight_{SSh}$} defines for \textbf{strain shift} for hands
\paragraph{(Density) $weight_{SSb}$} defines for \textbf{strain shift} for body 

Now what we need to do is to assign a reasonable values to these, and run the results to find our:
$$ difficulty := \lbrace(offset_1, difficulty_1), (offset_2, difficulty_2), ..., (offset_n, difficulty_n)\rbrace $$

Whereby we estimate difficulty at offset \textbf{n} from the map itself:

$$ difficulty_n \approx reading_n + \sum_{k=1}^{keys} \left(strain_k \right) $$








\end{document}